"""
FisherFaceäººè„¸è¯†åˆ«æ¨¡å‹å®ç°
"""

import numpy as np
import time
from tqdm import tqdm
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier

class EnhancedFisherFaceRecognizer:
    """
    å¢å¼ºç‰ˆFisherFaceè¯†åˆ«å™¨
    """
    def __init__(self, n_components=None, variance_ratio=0.98, knn_neighbors=5):
        """
        åˆå§‹åŒ–FisherFaceè¯†åˆ«å™¨
        """
        self.n_components = n_components
        self.variance_ratio = variance_ratio
        self.pca = None
        self.lda = None
        self.mean_face = None
        self.projected_faces = None
        self.train_labels = None
        self.training_time = None
        self.prediction_time = None
        self.knn = KNeighborsClassifier(n_neighbors=knn_neighbors, weights='distance')
        
    def train(self, train_images, train_labels):
        """
        è®­ç»ƒFisherFaceæ¨¡å‹
        """
        print("[INFO] ğŸš€ å¼€å§‹è®­ç»ƒFisherFaceæ¨¡å‹...")
        start_time = time.time()
        
        # æ­¥éª¤1: æ•°æ®é¢„å¤„ç†
        print("\nğŸ“Š æ­¥éª¤ 1/5: æ•°æ®é¢„å¤„ç†")
        n_samples = train_images.shape[0]
        n_classes = len(np.unique(train_labels))
        
        print(f"[INFO] è®­ç»ƒæ ·æœ¬æ•°: {n_samples}")
        print(f"[INFO] ç±»åˆ«æ•°: {n_classes}")
        print(f"[INFO] å›¾åƒå°ºå¯¸: {train_images.shape[1:]} -> ç‰¹å¾ç»´åº¦: {train_images.shape[1] * train_images.shape[2]}")
        
        # å°†å›¾åƒå±•å¹³ä¸ºå‘é‡
        print("[INFO] å±•å¹³å›¾åƒæ•°æ®...")
        flattened_images = []
        for i in tqdm(range(n_samples), desc="å±•å¹³å›¾åƒ", unit="å¼ "):
            flattened = train_images[i].reshape(-1).astype(np.float64)
            flattened_images.append(flattened)
        flattened_images = np.array(flattened_images)
        
        # æ­¥éª¤2: è®¡ç®—å¹³å‡è„¸
        print("\nğŸ‘¤ æ­¥éª¤ 2/5: è®¡ç®—å¹³å‡è„¸")
        print("[INFO] è®¡ç®—æ‰€æœ‰è®­ç»ƒå›¾åƒçš„å¹³å‡è„¸...")
        self.mean_face = np.mean(flattened_images, axis=0)
        print("âœ… å¹³å‡è„¸è®¡ç®—å®Œæˆ")
        
        # æ­¥éª¤3: æ•°æ®ä¸­å¿ƒåŒ–
        print("\nğŸ¯ æ­¥éª¤ 3/5: æ•°æ®ä¸­å¿ƒåŒ–")
        print("[INFO] ä»æ¯å¼ å›¾åƒä¸­å‡å»å¹³å‡è„¸...")
        centered_data = []
        for i in tqdm(range(len(flattened_images)), desc="ä¸­å¿ƒåŒ–å¤„ç†", unit="å¼ "):
            centered = flattened_images[i] - self.mean_face
            centered_data.append(centered)
        centered_data = np.array(centered_data)
        print("âœ… æ•°æ®ä¸­å¿ƒåŒ–å®Œæˆ")
        
        # æ­¥éª¤4: ç¡®å®šPCAç»„ä»¶æ•°
        print("\nğŸ”§ æ­¥éª¤ 4/5: ç¡®å®šPCAç»„ä»¶æ•°")
        if self.n_components is None:
            print("[INFO] è‡ªåŠ¨ç¡®å®šæœ€ä¼˜ç»„ä»¶æ•°...")
            print("[INFO] æ‰§è¡Œåˆå§‹PCAåˆ†æä»¥ç¡®å®šç»„ä»¶æ•°...")
            
            # ä½¿ç”¨è¾ƒå°çš„æ ·æœ¬è¿›è¡Œåˆå§‹åˆ†æä»¥èŠ‚çœæ—¶é—´
            sample_size = min(1000, len(centered_data))
            sample_indices = np.random.choice(len(centered_data), sample_size, replace=False)
            sample_data = centered_data[sample_indices]
            
            temp_pca = PCA()
            with tqdm(total=100, desc="PCAåˆ†æ", unit="%") as pbar:
                temp_pca.fit(sample_data)
                pbar.update(100)
            
            cumsum_ratio = np.cumsum(temp_pca.explained_variance_ratio_)
            self.n_components = np.argmax(cumsum_ratio >= self.variance_ratio) + 1
            self.n_components = min(self.n_components, n_samples - 1, 200)  # é™åˆ¶æœ€å¤§ç»„ä»¶æ•°
            
            print(f"[INFO] åŸºäº{self.variance_ratio*100:.1f}%æ–¹å·®ä¿ç•™ç¡®å®šç»„ä»¶æ•°: {self.n_components}")
        else:
            print(f"[INFO] ä½¿ç”¨é¢„è®¾ç»„ä»¶æ•°: {self.n_components}")
        
        # ä¸ºLDAè°ƒæ•´PCAç»„ä»¶æ•°
        self.n_components = min(self.n_components, n_samples - n_classes)
        print(f"[INFO] ä¸ºLDAè°ƒæ•´PCAç»„ä»¶æ•°: {self.n_components}")
        
        # æ­¥éª¤5: åº”ç”¨PCAé™ç»´
        print("\nğŸ¨ æ­¥éª¤ 5/5: PCAé™ç»´å’Œç‰¹å¾æŠ•å½±")
        print(f"[INFO] åº”ç”¨PCAé™ç»´: {flattened_images.shape[1]} -> {self.n_components} ç»´")
        
        self.pca = PCA(n_components=self.n_components, whiten=True)
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºPCAè®­ç»ƒè¿‡ç¨‹
        with tqdm(total=100, desc="PCAè®­ç»ƒ", unit="%") as pbar:
            pbar.set_description("è®¡ç®—åæ–¹å·®çŸ©é˜µ")
            pbar.update(20)
            
            pbar.set_description("ç‰¹å¾å€¼åˆ†è§£")
            pbar.update(30)
            
            # å®é™…PCAè®­ç»ƒ
            pca_projected = self.pca.fit_transform(centered_data)
            pbar.set_description("æŠ•å½±è®­ç»ƒæ•°æ®")
            pbar.update(50)
            
            pbar.set_description("PCAè®­ç»ƒå®Œæˆ")
            pbar.update(100)
        
        # åº”ç”¨LDA
        print("\n[INFO] åº”ç”¨LDAé™ç»´...")
        self.lda = LinearDiscriminantAnalysis()
        with tqdm(total=100, desc="LDAè®­ç»ƒ", unit="%") as pbar:
            self.projected_faces = self.lda.fit_transform(pca_projected, train_labels)
            pbar.update(100)
        
        self.train_labels = train_labels
        self.training_time = time.time() - start_time
        
        # è®­ç»ƒk-NNåˆ†ç±»å™¨
        print("\n[INFO] è®­ç»ƒk-NNåˆ†ç±»å™¨...")
        self.knn.fit(self.projected_faces, self.train_labels)
        
        # è¾“å‡ºè®­ç»ƒç»“æœ
        explained_variance = sum(self.lda.explained_variance_ratio_)
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ˆ LDAè§£é‡Šæ–¹å·®æ¯”ä¾‹: {explained_variance:.4f} ({explained_variance*100:.2f}%)")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {self.training_time:.2f}ç§’")
        print(f"ğŸ¯ æŠ•å½±åç‰¹å¾ç»´åº¦: {self.projected_faces.shape[1]}")
        print(f"ğŸ’¾ è®­ç»ƒæ•°æ®æŠ•å½±å½¢çŠ¶: {self.projected_faces.shape}")
        
    def predict(self, test_images):
        """
        é¢„æµ‹æµ‹è¯•å›¾åƒçš„æ ‡ç­¾
        """
        if self.pca is None or self.lda is None:
            raise ValueError("æ¨¡å‹å¿…é¡»å…ˆè®­ç»ƒæ‰èƒ½è¿›è¡Œé¢„æµ‹")
        
        print("\nğŸ”® å¼€å§‹é¢„æµ‹...")
        start_time = time.time()
        
        n_samples = test_images.shape[0]
        print(f"[INFO] é¢„æµ‹æ ·æœ¬æ•°: {n_samples}")
        
        # æ­¥éª¤1: é¢„å¤„ç†æµ‹è¯•å›¾åƒ
        print("\nğŸ“Š æ­¥éª¤ 1/3: é¢„å¤„ç†æµ‹è¯•å›¾åƒ")
        flattened_test = []
        for i in tqdm(range(n_samples), desc="å±•å¹³æµ‹è¯•å›¾åƒ", unit="å¼ "):
            flattened = test_images[i].reshape(-1).astype(np.float64)
            flattened_test.append(flattened)
        flattened_test = np.array(flattened_test)
        
        # æ­¥éª¤2: ä¸­å¿ƒåŒ–å¹¶æŠ•å½±
        print("\nğŸ¯ æ­¥éª¤ 2/3: ä¸­å¿ƒåŒ–å¹¶æŠ•å½±åˆ°ç‰¹å¾ç©ºé—´")
        centered_test = []
        for i in tqdm(range(len(flattened_test)), desc="ä¸­å¿ƒåŒ–å¤„ç†", unit="å¼ "):
            centered = flattened_test[i] - self.mean_face
            centered_test.append(centered)
        centered_test = np.array(centered_test)
        
        # æŠ•å½±åˆ°ç‰¹å¾ç©ºé—´
        print("[INFO] æŠ•å½±åˆ°PCAç‰¹å¾ç©ºé—´...")
        with tqdm(total=100, desc="PCAæŠ•å½±", unit="%") as pbar:
            projected_test = self.pca.transform(centered_test)
            pbar.update(100)
        
        print("[INFO] æŠ•å½±åˆ°LDAç‰¹å¾ç©ºé—´...")
        with tqdm(total=100, desc="LDAæŠ•å½±", unit="%") as pbar:
            projected_test = self.lda.transform(projected_test)
            pbar.update(100)
        
        # æ­¥éª¤3: k-NNåˆ†ç±»
        print("\nğŸ¯ æ­¥éª¤ 3/3: k-NNåˆ†ç±»")
        
        with tqdm(total=len(projected_test), desc="åˆ†ç±»é¢„æµ‹", unit="å¼ ") as pbar:
            predicted_labels = self.knn.predict(projected_test)
            pbar.update(len(projected_test))
        
        self.prediction_time = time.time() - start_time
        
        print(f"\nâœ… é¢„æµ‹å®Œæˆ!")
        print(f"â±ï¸  é¢„æµ‹æ—¶é—´: {self.prediction_time:.2f}ç§’")
        print(f"âš¡  å¹³å‡é¢„æµ‹é€Ÿåº¦: {n_samples/self.prediction_time:.2f} å¼ /ç§’")
        
        return np.array(predicted_labels)
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'n_components': self.n_components,
            'training_samples': len(self.train_labels) if self.train_labels is not None else 0,
            'training_time': self.training_time,
            'prediction_time': self.prediction_time,
            'explained_variance': sum(self.lda.explained_variance_ratio_) if self.lda else 0
        }
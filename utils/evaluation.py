"""
æ€§èƒ½è¯„ä¼°å·¥å…·
"""

import os
import numpy as np
import time
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

from config import get_report_path, get_plot_path
from .visualization import plot_confusion_matrix, plot_performance_metrics, show_prediction_examples
from .data_loader import align_face
import cv2

def comprehensive_evaluation(model, val_images, val_labels, label_map, model_name="Model"):
    """
    å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    print(f"\n{'='*60}")
    print(f"ğŸ¯ {model_name} æ¨¡å‹æ€§èƒ½è¯„ä¼°")
    print('='*60)
    
    # é¢„å¤„ç†éªŒè¯å›¾åƒ
    print("\n[INFO] é¢„å¤„ç†éªŒè¯å›¾åƒ...")
    processed_val_images = []
    for img in tqdm(val_images, desc="å¤„ç†éªŒè¯å›¾åƒ"):
        if len(img.shape) == 2:  # ç°åº¦å›¾åƒ
            aligned = align_face(img)
            equalized = cv2.equalizeHist(aligned)
            processed_val_images.append(equalized)
        else:  # å½©è‰²å›¾åƒ
            aligned = align_face(img)
            processed_val_images.append(aligned)
    val_images = np.array(processed_val_images)
    
    # é¢„æµ‹
    print("\nğŸ”® å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    evaluation_start = time.time()
    
    predictions = model.predict(val_images)
    
    print(f"\nğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    # è®¡ç®—å„ç§æŒ‡æ ‡
    with tqdm(total=100, desc="è®¡ç®—æŒ‡æ ‡", unit="%") as pbar:
        accuracy = accuracy_score(val_labels, predictions)
        pbar.update(25)
        
        precision = precision_score(val_labels, predictions, average='macro', zero_division=0)
        pbar.update(25)
        
        recall = recall_score(val_labels, predictions, average='macro', zero_division=0)
        pbar.update(25)
        
        f1 = f1_score(val_labels, predictions, average='macro', zero_division=0)
        pbar.update(25)
    
    # è®¡ç®—è¯¯æŠ¥ç‡
    cm = confusion_matrix(val_labels, predictions)
    
    # å¯¹äºå¤šåˆ†ç±»ï¼Œè®¡ç®—å¹³å‡è¯¯æŠ¥ç‡
    fp_rates = []
    for i in range(len(np.unique(val_labels))):
        tp = cm[i, i]  # çœŸæ­£ä¾‹
        fp = cm[:, i].sum() - tp  # å‡æ­£ä¾‹
        tn = cm.sum() - cm[i, :].sum() - cm[:, i].sum() + tp  # çœŸè´Ÿä¾‹
        fn = cm[i, :].sum() - tp  # å‡è´Ÿä¾‹
        
        if (fp + tn) > 0:
            fpr = fp / (fp + tn)
            fp_rates.append(fpr)
    
    avg_fpr = np.mean(fp_rates) if fp_rates else 0
    
    evaluation_time = time.time() - evaluation_start
    
    # è¾“å‡ºå…³é”®æ€§èƒ½æŒ‡æ ‡
    print(f"\nğŸ“Š æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡:")
    print(f"âœ… è¯†åˆ«å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"ğŸ¯ ç²¾ç¡®ç‡: {precision:.4f} ({precision*100:.2f}%)")
    print(f"ğŸ“ˆ å¬å›ç‡: {recall:.4f} ({recall*100:.2f}%)")
    print(f"ğŸ† F1åˆ†æ•°: {f1:.4f} ({f1*100:.2f}%)")
    print(f"âš ï¸  è¯¯æŠ¥ç‡: {avg_fpr:.4f} ({avg_fpr*100:.2f}%)")
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    print(f"\nğŸ¨ ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
    plot_confusion_matrix(cm, label_map, val_labels, predictions, model_name)
    
    # åˆ†ææ€§èƒ½
    analyze_performance(val_labels, predictions, label_map)
    
    # æ˜¾ç¤ºæ­£ç¡®å’Œé”™è¯¯è¯†åˆ«çš„ç¤ºä¾‹
    show_prediction_examples(val_images, val_labels, predictions, label_map, model_name)
    
    # è¿”å›ç»“æœå­—å…¸
    results = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'false_positive_rate': avg_fpr,
        'evaluation_time': evaluation_time,
        'model_info': model.get_model_info() if hasattr(model, 'get_model_info') else {}
    }
    
    return results

def analyze_performance(true_labels, predictions, label_map):
    """åˆ†ææ¨¡å‹æ€§èƒ½"""
    print(f"\nğŸ” æ€§èƒ½åˆ†æ:")
    
    # æ‰¾å‡ºæœ€å¥½å’Œæœ€å·®çš„ç±»åˆ«
    unique_labels = np.unique(true_labels)
    class_accuracies = []
    
    for label in unique_labels:
        mask = true_labels == label
        if np.sum(mask) > 0:
            class_acc = accuracy_score(true_labels[mask], predictions[mask])
            class_accuracies.append((label, class_acc, np.sum(mask)))
    
    # æ’åº
    class_accuracies.sort(key=lambda x: x[1], reverse=True)
    
    print(f"è¡¨ç°æœ€å¥½çš„3ä¸ªç±»åˆ«:")
    for i, (label, acc, count) in enumerate(class_accuracies[:3]):
        name = label_map.get(label, f"Class_{label}")
        print(f"  {i+1}. {name}: {acc:.4f} ({count}ä¸ªæ ·æœ¬)")
    
    print(f"è¡¨ç°æœ€å·®çš„3ä¸ªç±»åˆ«:")
    for i, (label, acc, count) in enumerate(class_accuracies[-3:]):
        name = label_map.get(label, f"Class_{label}")
        print(f"  {i+1}. {name}: {acc:.4f} ({count}ä¸ªæ ·æœ¬)")

def save_results(results, model_name):
    """ä¿å­˜éªŒè¯ç»“æœåˆ°æ–‡ä»¶"""
    filename = get_report_path(f"{model_name.lower()}_validation_results")
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"{model_name} æ¨¡å‹éªŒè¯ç»“æœ\n")
        f.write("="*50 + "\n\n")
        
        f.write("æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡:\n")
        f.write(f"è¯†åˆ«å‡†ç¡®ç‡: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\n")
        f.write(f"ç²¾ç¡®ç‡: {results['precision']:.4f} ({results['precision']*100:.2f}%)\n")
        f.write(f"å¬å›ç‡: {results['recall']:.4f} ({results['recall']*100:.2f}%)\n")
        f.write(f"F1åˆ†æ•°: {results['f1_score']:.4f} ({results['f1_score']*100:.2f}%)\n")
        f.write(f"è¯¯æŠ¥ç‡: {results['false_positive_rate']:.4f} ({results['false_positive_rate']*100:.2f}%)\n\n")
        
        f.write(f"è¯„ä¼°æ—¶é—´: {results['evaluation_time']:.2f}ç§’\n")
        
        # æ·»åŠ æ¨¡å‹ä¿¡æ¯
        if 'model_info' in results and results['model_info']:
            f.write("\næ¨¡å‹ä¿¡æ¯:\n")
            for key, value in results['model_info'].items():
                f.write(f"{key}: {value}\n")
        
        f.write(f"éªŒè¯æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    print(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

def comprehensive_comparison(all_results, model_names):
    """
    è¿›è¡Œç»¼åˆçš„æ¨¡å‹å¯¹æ¯”åˆ†æ
    """
    print(f"\n{'='*80}")
    print("ğŸ“Š æ¨¡å‹ç»¼åˆå¯¹æ¯”åˆ†æ")
    print('='*80)
    
    # æ•´ç†å¯¹æ¯”æ•°æ®
    comparison_data = {}
    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'false_positive_rate']
    
    for metric in metrics:
        comparison_data[metric] = {}
        for model_name in model_names:
            if model_name in all_results:
                comparison_data[metric][model_name] = all_results[model_name].get(metric, 0)
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print(f"\nğŸ“‹ æ€§èƒ½å¯¹æ¯”è¡¨:")
    print("-" * 80)
    print(f"{'æ¨¡å‹':<12} {'å‡†ç¡®ç‡':<10} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•°':<10} {'è¯¯æŠ¥ç‡':<10}")
    print("-" * 80)
    
    for model_name in model_names:
        if model_name in all_results:
            results = all_results[model_name]
            print(f"{model_name:<12} "
                  f"{results['accuracy']:<10.4f} "
                  f"{results['precision']:<10.4f} "
                  f"{results['recall']:<10.4f} "
                  f"{results['f1_score']:<10.4f} "
                  f"{results['false_positive_rate']:<10.4f}")
    
    print("-" * 80)
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_models = {}
    for metric in metrics:
        if metric == 'false_positive_rate':  # è¯¯æŠ¥ç‡è¶Šä½è¶Šå¥½
            best_model = min(comparison_data[metric].items(), key=lambda x: x[1])
        else:  # å…¶ä»–æŒ‡æ ‡è¶Šé«˜è¶Šå¥½
            best_model = max(comparison_data[metric].items(), key=lambda x: x[1])
        best_models[metric] = best_model
    
    print(f"\nğŸ† å„æŒ‡æ ‡æœ€ä½³æ¨¡å‹:")
    for metric, (model_name, value) in best_models.items():
        print(f"  {metric.replace('_', ' ').title()}: {model_name} ({value:.4f})")
    
    # ç»¼åˆæ’å
    print(f"\nğŸ¥‡ ç»¼åˆæ€§èƒ½æ’å:")
    overall_scores = {}
    for model_name in model_names:
        if model_name in all_results:
            results = all_results[model_name]
            # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆç®€å•å¹³å‡ï¼Œè¯¯æŠ¥ç‡å–åï¼‰
            score = (results['accuracy'] + results['precision'] + 
                    results['recall'] + results['f1_score'] - 
                    results['false_positive_rate']) / 4
            overall_scores[model_name] = score
    
    ranked_models = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)
    for i, (model_name, score) in enumerate(ranked_models, 1):
        print(f"  {i}. {model_name}: {score:.4f}")
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    plot_comparison_charts(all_results, model_names)
    
    # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
    save_comparison_report(all_results, model_names, best_models, ranked_models)

def plot_comparison_charts(all_results, model_names):
    """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
    import matplotlib.pyplot as plt
    
    # æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å›¾
    metrics = ['accuracy', 'precision', 'recall', 'f1_score']
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()
    
    for i, metric in enumerate(metrics):
        values = [all_results[model].get(metric, 0) for model in model_names if model in all_results]
        models = [model for model in model_names if model in all_results]
        
        bars = axes[i].bar(models, values)
        axes[i].set_title(f'{metric.replace("_", " ").title()} Comparison')
        axes[i].set_ylim(0, 1)
        axes[i].grid(axis='y', alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            axes[i].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                        f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(get_plot_path('performance_comparison'))
    plt.show()

def save_comparison_report(all_results, model_names, best_models, ranked_models):
    """ä¿å­˜å¯¹æ¯”æŠ¥å‘Š"""
    filename = get_report_path('comparison_report')
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("æ¨¡å‹ç»¼åˆå¯¹æ¯”æŠ¥å‘Š\n")
        f.write("="*50 + "\n\n")
        
        f.write("å‚ä¸å¯¹æ¯”çš„æ¨¡å‹:\n")
        for model_name in model_names:
            if model_name in all_results:
                f.write(f"- {model_name}\n")
        f.write("\n")
        
        f.write("æ€§èƒ½å¯¹æ¯”è¡¨:\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'æ¨¡å‹':<12} {'å‡†ç¡®ç‡':<10} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•°':<10} {'è¯¯æŠ¥ç‡':<10}\n")
        f.write("-" * 80 + "\n")
        
        for model_name in model_names:
            if model_name in all_results:
                results = all_results[model_name]
                f.write(f"{model_name:<12} "
                       f"{results['accuracy']:<10.4f} "
                       f"{results['precision']:<10.4f} "
                       f"{results['recall']:<10.4f} "
                       f"{results['f1_score']:<10.4f} "
                       f"{results['false_positive_rate']:<10.4f}\n")
        
        f.write("-" * 80 + "\n\n")
        
        f.write("å„æŒ‡æ ‡æœ€ä½³æ¨¡å‹:\n")
        for metric, (model_name, value) in best_models.items():
            f.write(f"{metric.replace('_', ' ').title()}: {model_name} ({value:.4f})\n")
        f.write("\n")
        
        f.write("ç»¼åˆæ€§èƒ½æ’å:\n")
        for i, (model_name, score) in enumerate(ranked_models, 1):
            f.write(f"{i}. {model_name}: {score:.4f}\n")
        
        f.write(f"\næŠ¥å‘Šç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    print(f"ğŸ“„ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")